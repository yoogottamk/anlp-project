{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"German to English translator system Explore the docs \u00bb Run demo WebAPP Report Bug Request Feature Table of Contents Team Information About The Project Built With Getting Started Prerequisites Installation Usage Contributing License Team Information \u00b6 Name Roll Number Branch Gaurang Tandon 2018101091 CSE Yoogottam Khandelwal 2018101019 CSE About The Project \u00b6 Webapp IMAGE HERE \u00dcbersetzerin is our comprehensive and robust ML pipeline built for bidirectional text translation between German and English languages. Key features: A webapp to easily and interactively experiment with any user input sentence A readthedocs website to host complete API reference for our code Two machine-learning models (seq2seq and transformer based) used by us to perform the translation task Built With \u00b6 \ud83e\udd17 Huggingface Bootstrap MDBootstrap ( back to top ) Getting Started \u00b6 Repository structure \u00b6 . \u251c\u2500\u2500 anlp_project # code for our ML pipeline \u251c\u2500\u2500 docs # documentation for our project \u251c\u2500\u2500 README.md # you are here \u251c\u2500\u2500 scripts # script to download dataset easily \u251c\u2500\u2500 requirements.txt # pip install this \u2514\u2500\u2500 setup.py # used to setup our pip package for easy distributions The source code has been ordered in a logical manner instead of just putting everything into a single file. anlp_project \u251c\u2500\u2500 config.py # loads all model hyperparameters \u251c\u2500\u2500 datasets # module for dataset loader processes \u251c\u2500\u2500 hparams.yaml # centralized file for all model hyperparameters \u251c\u2500\u2500 inference.py # runs inference # TODO Prerequisites \u00b6 We use Python's pip package manager. Perform these steps in root directory of project: Install all dependencies: pip install -r requirements.txt Installation \u00b6 Training and inference \u00b6 Our package is provided as a Python module. Perform these steps in root directory of project: Install the python package pip install -e . Webapp \u00b6 cd webapp pip install -r requirements.txt python app.py ( back to top ) Usage \u00b6 Model training \u00b6 Seq2Seq training \u00b6 T5 training \u00b6 Webapp \u00b6 For more examples, please refer to the Documentation ( back to top ) Contributing \u00b6 We really appreciate more contributions to keep the spirit of open source thriving! Feel free to create issues or pull requests on whatever topic you see fit. Don't forget to give the project a star! Thanks again! ( back to top ) License \u00b6 Distributed under the MIT License. See LICENSE for more information. ( back to top )","title":"Home"},{"location":"#team-information","text":"Name Roll Number Branch Gaurang Tandon 2018101091 CSE Yoogottam Khandelwal 2018101019 CSE","title":"Team Information"},{"location":"#about-the-project","text":"Webapp IMAGE HERE \u00dcbersetzerin is our comprehensive and robust ML pipeline built for bidirectional text translation between German and English languages. Key features: A webapp to easily and interactively experiment with any user input sentence A readthedocs website to host complete API reference for our code Two machine-learning models (seq2seq and transformer based) used by us to perform the translation task","title":"About The Project"},{"location":"#built-with","text":"\ud83e\udd17 Huggingface Bootstrap MDBootstrap ( back to top )","title":"Built With"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#repository-structure","text":". \u251c\u2500\u2500 anlp_project # code for our ML pipeline \u251c\u2500\u2500 docs # documentation for our project \u251c\u2500\u2500 README.md # you are here \u251c\u2500\u2500 scripts # script to download dataset easily \u251c\u2500\u2500 requirements.txt # pip install this \u2514\u2500\u2500 setup.py # used to setup our pip package for easy distributions The source code has been ordered in a logical manner instead of just putting everything into a single file. anlp_project \u251c\u2500\u2500 config.py # loads all model hyperparameters \u251c\u2500\u2500 datasets # module for dataset loader processes \u251c\u2500\u2500 hparams.yaml # centralized file for all model hyperparameters \u251c\u2500\u2500 inference.py # runs inference # TODO","title":"Repository structure"},{"location":"#prerequisites","text":"We use Python's pip package manager. Perform these steps in root directory of project: Install all dependencies: pip install -r requirements.txt","title":"Prerequisites"},{"location":"#installation","text":"","title":"Installation"},{"location":"#training-and-inference","text":"Our package is provided as a Python module. Perform these steps in root directory of project: Install the python package pip install -e .","title":"Training and inference"},{"location":"#webapp","text":"cd webapp pip install -r requirements.txt python app.py ( back to top )","title":"Webapp"},{"location":"#usage","text":"","title":"Usage"},{"location":"#model-training","text":"","title":"Model training"},{"location":"#seq2seq-training","text":"","title":"Seq2Seq training"},{"location":"#t5-training","text":"","title":"T5 training"},{"location":"#webapp_1","text":"For more examples, please refer to the Documentation ( back to top )","title":"Webapp"},{"location":"#contributing","text":"We really appreciate more contributions to keep the spirit of open source thriving! Feel free to create issues or pull requests on whatever topic you see fit. Don't forget to give the project a star! Thanks again! ( back to top )","title":"Contributing"},{"location":"#license","text":"Distributed under the MIT License. See LICENSE for more information. ( back to top )","title":"License"},{"location":"install/","text":"Installation \u00b6 You can install this project using pip : pip install git+https://github.com/yoogottamk/anlp-project.git","title":"Getting Started"},{"location":"install/#installation","text":"You can install this project using pip : pip install git+https://github.com/yoogottamk/anlp-project.git","title":"Installation"},{"location":"anlp_project/config/","text":"Config \u00b6 Global model configuration Collects defaults from hparams.yaml and overrides them with CLI arguments Available everywhere: model, dataset, trainer, etc dump ( self ) \u00b6 Prints all properties for this object Source code in anlp_project/config.py def dump ( self ): \"\"\" Prints all properties for this object \"\"\" pprint ( self . __dict__ ) from_file ( file_path = PosixPath ( '/home/yog/iiit/anlp/project/anlp_project/hparams.yaml' )) staticmethod \u00b6 Reads configuration from file Source code in anlp_project/config.py @staticmethod def from_file ( file_path = Path ( __file__ ) . parent / Path ( \"hparams.yaml\" )): \"\"\" Reads configuration from file \"\"\" c = Config () hparams = safe_load ( Path ( file_path ) . read_text ())[ \"hparams\" ] for hp in hparams : hp_name = hp [ \"name\" ] . replace ( \"-\" , \"_\" ) hp_type = locate ( hp [ \"type\" ]) if \"default\" in hp : hp_value = hp_type ( hp [ \"default\" ]) else : hp_value = None c . __setattr__ ( hp_name , hp_value ) return c","title":"config"},{"location":"anlp_project/config/#anlp_project.config.Config","text":"Global model configuration Collects defaults from hparams.yaml and overrides them with CLI arguments Available everywhere: model, dataset, trainer, etc","title":"Config"},{"location":"anlp_project/config/#anlp_project.config.Config.dump","text":"Prints all properties for this object Source code in anlp_project/config.py def dump ( self ): \"\"\" Prints all properties for this object \"\"\" pprint ( self . __dict__ )","title":"dump()"},{"location":"anlp_project/config/#anlp_project.config.Config.from_file","text":"Reads configuration from file Source code in anlp_project/config.py @staticmethod def from_file ( file_path = Path ( __file__ ) . parent / Path ( \"hparams.yaml\" )): \"\"\" Reads configuration from file \"\"\" c = Config () hparams = safe_load ( Path ( file_path ) . read_text ())[ \"hparams\" ] for hp in hparams : hp_name = hp [ \"name\" ] . replace ( \"-\" , \"_\" ) hp_type = locate ( hp [ \"type\" ]) if \"default\" in hp : hp_value = hp_type ( hp [ \"default\" ]) else : hp_value = None c . __setattr__ ( hp_name , hp_value ) return c","title":"from_file()"},{"location":"anlp_project/inference/","text":"inference_model ( config , checkpoint_file , input_sentence ) \u00b6 Inference for rudimentary seq2seq model Source code in anlp_project/inference.py def inference_model ( config : Config , checkpoint_file : str , input_sentence : str ): \"\"\" Inference for rudimentary seq2seq model \"\"\" if not input_sentence : # parliament related sample sentence # it is German for: \"Our citizens need better water supply to their house\" input_sentence = ( \"unsere burger brauchen eine bessere wasserversorgung ihres hauses\" ) dataset = EuroParl ( config = config ) # input is English, output is German input_size = dataset . de_vocab_size output_size = dataset . en_vocab_size logging . info ( \"Input size (German vocab): %d ; Output size (English vocab): %d \" , input_size , output_size , ) model = Seq2SeqRNN ( config , input_size , output_size ) model . load_state_dict ( torch . load ( checkpoint_file )[ \"state_dict\" ]) model . eval () logging . info ( \"Parameters of loaded model: input/output size: %d / %d \" , model . input_size , model . output_size , ) token_sentence = dataset . sentence_to_indices ( input_sentence ) token_sentence = [ dataset . BOS_TOKEN_INDEX , * token_sentence , dataset . EOS_TOKEN_INDEX ] decoded_words_tokens = model . evaluate ( token_sentence ) # convert token integers back to words decoded_sentence = dataset . indices_to_sentence ( decoded_words_tokens ) print ( decoded_sentence ) inference_t5 ( checkpoint_file , input_sentence ) \u00b6 Inference for baseline++ model Source code in anlp_project/inference.py def inference_t5 ( checkpoint_file : str , input_sentence : str ): \"\"\" Inference for baseline++ model \"\"\" # Initialize the tokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint_file ) # Initialize the model model = AutoModelForSeq2SeqLM . from_pretrained ( checkpoint_file ) text = f \"translate German to English: { input_sentence } \" tokenized_text = tokenizer ([ text ], return_tensors = \"pt\" , padding = True ) # Perform translation and decode the output translation = model . generate ( ** tokenized_text ) translated_text = tokenizer . batch_decode ( translation , skip_special_tokens = True ) return translated_text [ 0 ]","title":"inference"},{"location":"anlp_project/inference/#anlp_project.inference.inference_model","text":"Inference for rudimentary seq2seq model Source code in anlp_project/inference.py def inference_model ( config : Config , checkpoint_file : str , input_sentence : str ): \"\"\" Inference for rudimentary seq2seq model \"\"\" if not input_sentence : # parliament related sample sentence # it is German for: \"Our citizens need better water supply to their house\" input_sentence = ( \"unsere burger brauchen eine bessere wasserversorgung ihres hauses\" ) dataset = EuroParl ( config = config ) # input is English, output is German input_size = dataset . de_vocab_size output_size = dataset . en_vocab_size logging . info ( \"Input size (German vocab): %d ; Output size (English vocab): %d \" , input_size , output_size , ) model = Seq2SeqRNN ( config , input_size , output_size ) model . load_state_dict ( torch . load ( checkpoint_file )[ \"state_dict\" ]) model . eval () logging . info ( \"Parameters of loaded model: input/output size: %d / %d \" , model . input_size , model . output_size , ) token_sentence = dataset . sentence_to_indices ( input_sentence ) token_sentence = [ dataset . BOS_TOKEN_INDEX , * token_sentence , dataset . EOS_TOKEN_INDEX ] decoded_words_tokens = model . evaluate ( token_sentence ) # convert token integers back to words decoded_sentence = dataset . indices_to_sentence ( decoded_words_tokens ) print ( decoded_sentence )","title":"inference_model()"},{"location":"anlp_project/inference/#anlp_project.inference.inference_t5","text":"Inference for baseline++ model Source code in anlp_project/inference.py def inference_t5 ( checkpoint_file : str , input_sentence : str ): \"\"\" Inference for baseline++ model \"\"\" # Initialize the tokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint_file ) # Initialize the model model = AutoModelForSeq2SeqLM . from_pretrained ( checkpoint_file ) text = f \"translate German to English: { input_sentence } \" tokenized_text = tokenizer ([ text ], return_tensors = \"pt\" , padding = True ) # Perform translation and decode the output translation = model . generate ( ** tokenized_text ) translated_text = tokenizer . batch_decode ( translation , skip_special_tokens = True ) return translated_text [ 0 ]","title":"inference_t5()"},{"location":"anlp_project/main/","text":"anlp_project ( ** kwargs ) \u00b6 Entrypoint for anlp_project CLI Source code in anlp_project/main.py @cli_decorator def anlp_project ( ** kwargs ): \"\"\" Entrypoint for `anlp_project` CLI \"\"\" config = Config ( ** kwargs ) logging . basicConfig ( level = logging . DEBUG , format = \" %(message)s \" , datefmt = \"[ %X ]\" , handlers = [ RichHandler ()], ) if not kwargs . get ( \"disable_print_config\" , False ): config . dump () if kwargs [ \"subcmd\" ] == \"train\" : train_model ( config ) if kwargs [ \"subcmd\" ] == \"inference\" : print ( inference_t5 ( kwargs [ \"checkpoint\" ], kwargs [ \"sentence\" ]))","title":"main"},{"location":"anlp_project/main/#anlp_project.main.anlp_project","text":"Entrypoint for anlp_project CLI Source code in anlp_project/main.py @cli_decorator def anlp_project ( ** kwargs ): \"\"\" Entrypoint for `anlp_project` CLI \"\"\" config = Config ( ** kwargs ) logging . basicConfig ( level = logging . DEBUG , format = \" %(message)s \" , datefmt = \"[ %X ]\" , handlers = [ RichHandler ()], ) if not kwargs . get ( \"disable_print_config\" , False ): config . dump () if kwargs [ \"subcmd\" ] == \"train\" : train_model ( config ) if kwargs [ \"subcmd\" ] == \"inference\" : print ( inference_t5 ( kwargs [ \"checkpoint\" ], kwargs [ \"sentence\" ]))","title":"anlp_project()"},{"location":"anlp_project/train/","text":"train_model ( config ) \u00b6 Model trainer for seq2seq Source code in anlp_project/train.py def train_model ( config : Config ): \"\"\" Model trainer for seq2seq \"\"\" dataset = EuroParl ( config = config , force_regenerate_mappings = True ) # input is English, output is German input_size = dataset . de_vocab_size output_size = dataset . en_vocab_size logging . info ( \"Input size (German vocab size): %d ; Output size (English vocab): %d \" , input_size , output_size , ) model = Seq2SeqRNN ( config , input_size , output_size ) total_entries = len ( dataset ) train_ratio = 0.8 train_length = int ( total_entries * train_ratio ) # bad hack to make this work # always remain a multiple of 4 train_length -= train_length % 16 test_length = total_entries - train_length cpu_count = int ( os . getenv ( \"SLURM_CPUS_ON_NODE\" , str ( multiprocessing . cpu_count ()))) train_data , test_data = random_split ( dataset , [ train_length , test_length ], generator = torch . Generator () . manual_seed ( 42 ), ) train_dataloader = DataLoader ( train_data , batch_size = config . batch_size , shuffle = True , num_workers = cpu_count ) # do not shuffle validation dataloader val_dataloader = DataLoader ( test_data , batch_size = config . batch_size , num_workers = cpu_count ) wandb_logger = WandbLogger () # -1 implies use all GPUs available gpu_count = - 1 if torch . cuda . is_available () else 0 checkpoint_path = get_checkpoint_dir ( config ) trainer = Trainer ( logger = wandb_logger if config . log_wandb else [], callbacks = [ ModelCheckpoint ( dirpath = checkpoint_path , every_n_epochs = 1 )], max_epochs = config . n_epochs , gpus = gpu_count , strategy = \"ddp\" , default_root_dir = checkpoint_path , ) trainer . fit ( model , train_dataloader , val_dataloader )","title":"train"},{"location":"anlp_project/train/#anlp_project.train.train_model","text":"Model trainer for seq2seq Source code in anlp_project/train.py def train_model ( config : Config ): \"\"\" Model trainer for seq2seq \"\"\" dataset = EuroParl ( config = config , force_regenerate_mappings = True ) # input is English, output is German input_size = dataset . de_vocab_size output_size = dataset . en_vocab_size logging . info ( \"Input size (German vocab size): %d ; Output size (English vocab): %d \" , input_size , output_size , ) model = Seq2SeqRNN ( config , input_size , output_size ) total_entries = len ( dataset ) train_ratio = 0.8 train_length = int ( total_entries * train_ratio ) # bad hack to make this work # always remain a multiple of 4 train_length -= train_length % 16 test_length = total_entries - train_length cpu_count = int ( os . getenv ( \"SLURM_CPUS_ON_NODE\" , str ( multiprocessing . cpu_count ()))) train_data , test_data = random_split ( dataset , [ train_length , test_length ], generator = torch . Generator () . manual_seed ( 42 ), ) train_dataloader = DataLoader ( train_data , batch_size = config . batch_size , shuffle = True , num_workers = cpu_count ) # do not shuffle validation dataloader val_dataloader = DataLoader ( test_data , batch_size = config . batch_size , num_workers = cpu_count ) wandb_logger = WandbLogger () # -1 implies use all GPUs available gpu_count = - 1 if torch . cuda . is_available () else 0 checkpoint_path = get_checkpoint_dir ( config ) trainer = Trainer ( logger = wandb_logger if config . log_wandb else [], callbacks = [ ModelCheckpoint ( dirpath = checkpoint_path , every_n_epochs = 1 )], max_epochs = config . n_epochs , gpus = gpu_count , strategy = \"ddp\" , default_root_dir = checkpoint_path , ) trainer . fit ( model , train_dataloader , val_dataloader )","title":"train_model()"},{"location":"anlp_project/utils/","text":"","title":"utils"},{"location":"anlp_project/datasets/convert-to-jsonl/","text":"main () \u00b6 Converts dataset to jsonlines format needed for baseline++ training Source code in datasets/convert-to-jsonl.py def main (): \"\"\" Converts dataset to jsonlines format needed for baseline++ training \"\"\" dataset = EuroParlRaw () with open ( \"dataset.jsonl\" , \"w\" ) as f : for de , en in tqdm ( dataset ): f . write ( json . dumps ({ \"translation\" : { \"en\" : en , \"de\" : de }}) + \" \\n \" )","title":"convert-to-jsonl"},{"location":"anlp_project/datasets/convert-to-jsonl/#anlp_project.datasets.convert-to-jsonl.main","text":"Converts dataset to jsonlines format needed for baseline++ training Source code in datasets/convert-to-jsonl.py def main (): \"\"\" Converts dataset to jsonlines format needed for baseline++ training \"\"\" dataset = EuroParlRaw () with open ( \"dataset.jsonl\" , \"w\" ) as f : for de , en in tqdm ( dataset ): f . write ( json . dumps ({ \"translation\" : { \"en\" : en , \"de\" : de }}) + \" \\n \" )","title":"main()"},{"location":"anlp_project/datasets/europarl/","text":"EuroParl ( EuroParlRaw ) \u00b6 Processed dataset Returns tensors after padding EuroParlRaw ( Dataset ) \u00b6 Raw dataset: tuples of (de, en) pairs","title":"europarl"},{"location":"anlp_project/datasets/europarl/#anlp_project.datasets.europarl.EuroParl","text":"Processed dataset Returns tensors after padding","title":"EuroParl"},{"location":"anlp_project/datasets/europarl/#anlp_project.datasets.europarl.EuroParlRaw","text":"Raw dataset: tuples of (de, en) pairs","title":"EuroParlRaw"},{"location":"anlp_project/models/seq2seq/","text":"AttentionDecoderRNN ( Module ) \u00b6 Module for attention based RNN Decoder forward ( self , input , hidden , encoder_outputs ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden , encoder_outputs ): batch_size = input . size ( 0 ) emb = self . embedding ( input ) . view ( 1 , batch_size , - 1 ) emb = self . dropout ( emb ) # shape is (batch, 2 * hidden_size) # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv # shape is (batch, sentence max length) # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv attn_weights = F . softmax ( self . attn ( torch . cat (( emb [ 0 ], hidden [ 0 ]), 1 )), dim = 1 ) # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ # shape is (batch, max length of sentence) # encoder_outputs = (batch, max length of sentence, hidden size) # attn_weights: (batch, 1, max length of sentence) attn_weights = attn_weights . unsqueeze ( 1 ) # bmm == batch matrix matrix product attn_applied = torch . bmm ( attn_weights , encoder_outputs ) . view ( 1 , batch_size , - 1 ) output = torch . cat (( emb [ 0 ], attn_applied [ 0 ]), 1 ) output = self . attn_combine ( output ) . unsqueeze ( 0 ) output = F . relu ( output ) # output is (1, BATCH_SIZE, HIDDEN_SIZE) output , hidden = self . gru ( output , hidden ) output = self . output_with_activation ( output [ 0 ]) return output , hidden , attn_weights DecoderRNN ( Module ) \u00b6 Module for simple RNN Decoder forward ( self , input , hidden ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden ): emb = self . emb_layer_with_activation ( input ) . view ( 1 , input . size ( 0 ), - 1 ) output , hidden = self . gru ( emb , hidden ) output = self . output_with_activation ( output [ 0 ]) return output , hidden EncoderRNN ( Module ) \u00b6 Module for RNN Encoder forward ( self , input , hidden ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden ): emb_i = self . embedding ( input ) # GRU expects L, N, H # since we're passing a single word, L=1 # N = batch size # H = hidden_state size for us emb = emb_i . view ( 1 , input . size ( 0 ), - 1 ) output , hidden = self . gru ( emb , hidden ) return output , hidden Seq2SeqRNN ( LightningModule ) \u00b6 Seq2Seq Module, uses Encoder and attention Decoder configure_optimizers ( self ) \u00b6 Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Any of these 6 options. Single optimizer . List or Tuple of optimizers. Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_scheduler_config ). Dictionary , with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config . Tuple of dictionaries as described above, with an optional \"frequency\" key. None - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the :class: torch.optim.lr_scheduler.ReduceLROnPlateau scheduler, Lightning requires that the lr_scheduler_config contains the keyword \"monitor\" set to the metric name that the scheduler should be conditioned on. .. testcode:: # The ReduceLROnPlateau scheduler requires a monitor def configure_optimizers(self): optimizer = Adam(...) return { \"optimizer\": optimizer, \"lr_scheduler\": { \"scheduler\": ReduceLROnPlateau(optimizer, ...), \"monitor\": \"metric_to_track\", \"frequency\": \"indicates how often the metric is updated\" # If \"monitor\" references validation metrics, then \"frequency\" should be set to a # multiple of \"trainer.check_val_every_n_epoch\". }, } # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler def configure_optimizers(self): optimizer1 = Adam(...) optimizer2 = SGD(...) scheduler1 = ReduceLROnPlateau(optimizer1, ...) scheduler2 = LambdaLR(optimizer2, ...) return ( { \"optimizer\": optimizer1, \"lr_scheduler\": { \"scheduler\": scheduler1, \"monitor\": \"metric_to_track\", }, }, {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2}, ) Metrics can be made available to monitor by simply logging it using self.log('metric_to_track', metric_val) in your :class: ~pytorch_lightning.core.lightning.LightningModule . !!! note The frequency value specified in a dict along with the optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: - In the former case, all optimizers will operate on the given batch in each optimization step. - In the latter, only one optimizer will operate on the given batch at every step. This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above. .. code-block:: python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {\"optimizer\": optimizer_one, \"frequency\": 5}, {\"optimizer\": optimizer_two, \"frequency\": 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples:: # most cases. no learning rate scheduler def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3) # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt # example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch] # example with step-based learning rate schedulers # each optimizer has its own scheduler def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = { 'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step' # called after each training step } dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) !!! note Some things to know: - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers. - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter. - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default ``.step()`` schedule, override the :meth:`optimizer_step` hook. Source code in models/seq2seq.py def configure_optimizers ( self ): enc_opt = optim . Adam ( self . encoder . parameters (), self . config . lr ) dec_opt = optim . Adam ( self . decoder . parameters (), self . config . lr ) return enc_opt , dec_opt training_step ( self , batch , _batch_idx ) \u00b6 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx ``int`` Integer displaying index of this batch required optimizer_idx ``int`` When using multiple optimizers, this argument will also be present. required hiddens ``Any`` Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} !!! note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in models/seq2seq.py def training_step ( self , batch , _batch_idx ): enc_optim , dec_optim = self . optimizers () ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) = self . _move_encoder_forward ( batch ) use_teacher_forcing = random () < self . config . teacher_forcing_ratio loss = 0 if use_teacher_forcing : loss_function = nn . NLLLoss () for word_index in range ( target_word_count ): decoder_output , decoder_hidden , _ = self . decoder ( decoder_input , decoder_hidden , encoder_outputs ) loss += loss_function ( decoder_output , target_tensor [:, word_index ]) decoder_input = target_tensor [:, word_index ] . unsqueeze ( 1 ) else : loss = self . _move_decoder_forward ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) loss . backward () enc_optim . step () dec_optim . step () train_loss = loss . item () / target_word_count self . log ( \"train_loss\" , train_loss , on_step = True , on_epoch = True , prog_bar = True ) return { \"train_loss\" : train_loss } validation_step ( self , batch , batch_idx ) \u00b6 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to validate you don't need to implement this method. !!! note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in models/seq2seq.py def validation_step ( self , batch , batch_idx ): ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) = self . _move_encoder_forward ( batch ) loss = self . _move_decoder_forward ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) val_loss = loss . item () / target_word_count self . log ( \"validation_loss\" , val_loss , on_step = True , on_epoch = True , prog_bar = True ) return val_loss","title":"seq2seq"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.AttentionDecoderRNN","text":"Module for attention based RNN Decoder","title":"AttentionDecoderRNN"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.AttentionDecoderRNN.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden , encoder_outputs ): batch_size = input . size ( 0 ) emb = self . embedding ( input ) . view ( 1 , batch_size , - 1 ) emb = self . dropout ( emb ) # shape is (batch, 2 * hidden_size) # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv # shape is (batch, sentence max length) # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv attn_weights = F . softmax ( self . attn ( torch . cat (( emb [ 0 ], hidden [ 0 ]), 1 )), dim = 1 ) # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ # shape is (batch, max length of sentence) # encoder_outputs = (batch, max length of sentence, hidden size) # attn_weights: (batch, 1, max length of sentence) attn_weights = attn_weights . unsqueeze ( 1 ) # bmm == batch matrix matrix product attn_applied = torch . bmm ( attn_weights , encoder_outputs ) . view ( 1 , batch_size , - 1 ) output = torch . cat (( emb [ 0 ], attn_applied [ 0 ]), 1 ) output = self . attn_combine ( output ) . unsqueeze ( 0 ) output = F . relu ( output ) # output is (1, BATCH_SIZE, HIDDEN_SIZE) output , hidden = self . gru ( output , hidden ) output = self . output_with_activation ( output [ 0 ]) return output , hidden , attn_weights","title":"forward()"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.DecoderRNN","text":"Module for simple RNN Decoder","title":"DecoderRNN"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.DecoderRNN.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden ): emb = self . emb_layer_with_activation ( input ) . view ( 1 , input . size ( 0 ), - 1 ) output , hidden = self . gru ( emb , hidden ) output = self . output_with_activation ( output [ 0 ]) return output , hidden","title":"forward()"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.EncoderRNN","text":"Module for RNN Encoder","title":"EncoderRNN"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.EncoderRNN.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in models/seq2seq.py def forward ( self , input , hidden ): emb_i = self . embedding ( input ) # GRU expects L, N, H # since we're passing a single word, L=1 # N = batch size # H = hidden_state size for us emb = emb_i . view ( 1 , input . size ( 0 ), - 1 ) output , hidden = self . gru ( emb , hidden ) return output , hidden","title":"forward()"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.Seq2SeqRNN","text":"Seq2Seq Module, uses Encoder and attention Decoder","title":"Seq2SeqRNN"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.Seq2SeqRNN.configure_optimizers","text":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Any of these 6 options. Single optimizer . List or Tuple of optimizers. Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_scheduler_config ). Dictionary , with an \"optimizer\" key, and (optionally) a \"lr_scheduler\" key whose value is a single LR scheduler or lr_scheduler_config . Tuple of dictionaries as described above, with an optional \"frequency\" key. None - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the :class: torch.optim.lr_scheduler.ReduceLROnPlateau scheduler, Lightning requires that the lr_scheduler_config contains the keyword \"monitor\" set to the metric name that the scheduler should be conditioned on. .. testcode:: # The ReduceLROnPlateau scheduler requires a monitor def configure_optimizers(self): optimizer = Adam(...) return { \"optimizer\": optimizer, \"lr_scheduler\": { \"scheduler\": ReduceLROnPlateau(optimizer, ...), \"monitor\": \"metric_to_track\", \"frequency\": \"indicates how often the metric is updated\" # If \"monitor\" references validation metrics, then \"frequency\" should be set to a # multiple of \"trainer.check_val_every_n_epoch\". }, } # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler def configure_optimizers(self): optimizer1 = Adam(...) optimizer2 = SGD(...) scheduler1 = ReduceLROnPlateau(optimizer1, ...) scheduler2 = LambdaLR(optimizer2, ...) return ( { \"optimizer\": optimizer1, \"lr_scheduler\": { \"scheduler\": scheduler1, \"monitor\": \"metric_to_track\", }, }, {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2}, ) Metrics can be made available to monitor by simply logging it using self.log('metric_to_track', metric_val) in your :class: ~pytorch_lightning.core.lightning.LightningModule . !!! note The frequency value specified in a dict along with the optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: - In the former case, all optimizers will operate on the given batch in each optimization step. - In the latter, only one optimizer will operate on the given batch at every step. This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above. .. code-block:: python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {\"optimizer\": optimizer_one, \"frequency\": 5}, {\"optimizer\": optimizer_two, \"frequency\": 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples:: # most cases. no learning rate scheduler def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3) # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt # example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch] # example with step-based learning rate schedulers # each optimizer has its own scheduler def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = { 'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step' # called after each training step } dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) !!! note Some things to know: - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers. - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter. - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default ``.step()`` schedule, override the :meth:`optimizer_step` hook. Source code in models/seq2seq.py def configure_optimizers ( self ): enc_opt = optim . Adam ( self . encoder . parameters (), self . config . lr ) dec_opt = optim . Adam ( self . decoder . parameters (), self . config . lr ) return enc_opt , dec_opt","title":"configure_optimizers()"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.Seq2SeqRNN.training_step","text":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx ``int`` Integer displaying index of this batch required optimizer_idx ``int`` When using multiple optimizers, this argument will also be present. required hiddens ``Any`` Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} !!! note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in models/seq2seq.py def training_step ( self , batch , _batch_idx ): enc_optim , dec_optim = self . optimizers () ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) = self . _move_encoder_forward ( batch ) use_teacher_forcing = random () < self . config . teacher_forcing_ratio loss = 0 if use_teacher_forcing : loss_function = nn . NLLLoss () for word_index in range ( target_word_count ): decoder_output , decoder_hidden , _ = self . decoder ( decoder_input , decoder_hidden , encoder_outputs ) loss += loss_function ( decoder_output , target_tensor [:, word_index ]) decoder_input = target_tensor [:, word_index ] . unsqueeze ( 1 ) else : loss = self . _move_decoder_forward ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) loss . backward () enc_optim . step () dec_optim . step () train_loss = loss . item () / target_word_count self . log ( \"train_loss\" , train_loss , on_step = True , on_epoch = True , prog_bar = True ) return { \"train_loss\" : train_loss }","title":"training_step()"},{"location":"anlp_project/models/seq2seq/#anlp_project.models.seq2seq.Seq2SeqRNN.validation_step","text":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... !!! note If you don't need to validate you don't need to implement this method. !!! note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in models/seq2seq.py def validation_step ( self , batch , batch_idx ): ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) = self . _move_encoder_forward ( batch ) loss = self . _move_decoder_forward ( decoder_input , decoder_hidden , target_tensor , target_word_count , encoder_outputs , ) val_loss = loss . item () / target_word_count self . log ( \"validation_loss\" , val_loss , on_step = True , on_epoch = True , prog_bar = True ) return val_loss","title":"validation_step()"},{"location":"anlp_project/models/transformer/","text":"DataTrainingArguments dataclass \u00b6 Arguments pertaining to what data we are going to input our model for training and eval. ModelArguments dataclass \u00b6 Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.","title":"transformer"},{"location":"anlp_project/models/transformer/#anlp_project.models.transformer.DataTrainingArguments","text":"Arguments pertaining to what data we are going to input our model for training and eval.","title":"DataTrainingArguments"},{"location":"anlp_project/models/transformer/#anlp_project.models.transformer.ModelArguments","text":"Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.","title":"ModelArguments"}]}